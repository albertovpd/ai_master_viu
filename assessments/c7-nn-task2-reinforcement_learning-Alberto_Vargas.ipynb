{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inm59eH6tPUc"
   },
   "source": [
    "### Teoría"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YawQm9BtPUl"
   },
   "source": [
    "#### Define brevemente qué es el aprendizaje por refuerzo. ¿Qué diferencias hay entre aprendizaje supervisado, no supervisado y por refuerzo?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSv2LTEFc0vy"
   },
   "source": [
    "Definiendo brevemente, el aprendizaje por refuerzo es un método con el que se pretende optimizar el comportamiento de un agente autónomo para maximizar la recompensa que le asignemos, es decir, se *premia* al modelo cuanto más se acerque a la solución deseada durante una serie de iteraciones.\n",
    "\n",
    "- El aprendizaje supervisado consiste en realizar inferencias, teniendo acceso a datos con los que previamente entrenar nuestro modelo. \n",
    "- En no supervisado no tenemos esa suerte, y se necesita atacar el problema de otra manera (clustering con similitudes y distancias). Las técnicas de no supervisado a menudo se convierten en el preludio de técnicas de supervisado (porque no podemos realizar Supervised Learning si no tenemos datos con los que trabajar). \n",
    "- Finalmente, podemos decir que en RL tampoco tenemos datos para entrenar y el problema es diferente: Tenemos un objetivo y queremos encontrar la manera más óptima de llegar a él."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AStkAQUrtPUn"
   },
   "source": [
    "#### Define con tus palabras los conceptos de Entorno, Agente, Recompensa, Estado y Observación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRwq5TZitPUo"
   },
   "source": [
    "- Entorno: El universo de nuestro problema. Tiene sus propias características y restricciones. \n",
    "- Agente: El modelo con el que estamos trabajando.\n",
    "- Recompensa: Es cómo premiamos al agente cuando se acerca a la solución que consideramos satisfactoria. \n",
    "- Observación: Los parámetros el proyecto en un momento dado del espacio-tiempo del sistema.\n",
    "- Estado: Parámetros que involucran a nuestro agente y el entorno en un momento dado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJo9zpGFtPUo"
   },
   "source": [
    "#### Dependiendo del algoritmo de aprendizaje por refuerzo que se use, ¿qué clasificaciones podemos encontrar? Coméntalas brevemente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8-2Du2ytPUo"
   },
   "source": [
    "Según hemos visto, una buena primera clasificación pueden ser los algoritmos basados en la estrategia y los algoritmos basados en el modelo.\n",
    "\n",
    "- Estrategia: Determinan qué acción tomar en un estado. Se pueden clasificar en On-Policy (se intenta evaluar y mejorar la misma policy que el agente está usando para elegir una acción) y Off-Policy (la policy que se intenta mejorar es diferente de la usada para seleccionar la acción, como con Q learning).\n",
    "\n",
    "- Modelo: Existen dinámicas en el entorno que conocemos y las acciones que toma el agente están condicionadas por las mismas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTxc-ECDtPUp"
   },
   "source": [
    "#### Lista tres diferencias entre los algoritmos de DQN y Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0lc4x2ZtPUq"
   },
   "source": [
    "- Los algoritmos Policy Gradient buscan la mejor policy optimizando la recompensa.\n",
    "- Los Policy Gradient suelen tener una varianza alta.\n",
    "- Para entornos contínuos.\n",
    "- DQNs alcanzan la policy óptima aprendiendo cuáles son las mejores value functions. Alcanzan el mismo objetivo, de forma indirecta.\n",
    "- Suelen ser más rápidos y estables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6m_aO-7tPUq"
   },
   "source": [
    "### Práctica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YACQDv8tPUr"
   },
   "source": [
    "Algunas consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, una solución óptima será alcanzar una media de recompensa por encima de 16 puntos. Para medir si hemos conseguido llegar a la solución óptima, la media de la recompensa se calculará a partir del código de test en la última celda del notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dndmWyMAtPUu"
   },
   "source": [
    "Este bloque práctico consta de tres partes:\n",
    "\n",
    "   1) Implementar la red neuronal que se usará en la solución\n",
    "    \n",
    "   2) Seleccionar los hiperparámetros adecuados para las distintas piezas de la solución DQN\n",
    "    \n",
    "   3) Justificar la respuesta en relación a los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3BCQ7B6tPUv"
   },
   "source": [
    "IMPORTANTE:\n",
    "\n",
    "- Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "\n",
    "- Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "\n",
    "- Si usáis Google Colab, recordad usar las versiones de Tensorflow==1.13.1, Keras==2.2.4 y keras-rl==0.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRIGfltWtPUx",
    "outputId": "80aa69ed-a270-437f-a7e4-cbd3c5481250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.13.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/ea/ab2c8c0e81bd051cc1180b104c75a865ab0fc66c89be992c4b20bbf6d624/tensorflow-1.13.1-cp27-cp27mu-manylinux1_x86_64.whl (92.5MB)\n",
      "\u001b[K     |████████████████████████████████| 92.5MB 94kB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K     |████████████████████████████████| 368kB 41.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (2.0.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
      "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.1.6)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (3.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/ac/48dd71c2bdc8d31e367f9b72f25ccb3b89bc6b9d664fee21f9a8efa5714d/tensorboard-1.13.1-py2-none-any.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 32.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (0.7.1)\n",
      "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.0.post1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.16.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
      "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==1.13.1) (3.2.0)\n",
      "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.13.1) (1.0.2)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.13.1) (5.4.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (44.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.15.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
      "  Found existing installation: tensorflow-estimator 1.15.0\n",
      "    Uninstalling tensorflow-estimator-1.15.0:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.0\n",
      "  Found existing installation: tensorboard 2.1.0\n",
      "    Uninstalling tensorboard-2.1.0:\n",
      "      Successfully uninstalled tensorboard-2.1.0\n",
      "  Found existing installation: tensorflow 2.1.0\n",
      "    Uninstalling tensorflow-2.1.0:\n",
      "      Successfully uninstalled tensorflow-2.1.0\n",
      "Successfully installed tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n",
      "Collecting keras==2.2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 5.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (1.16.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (1.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (1.2.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
      "Installing collected packages: keras\n",
      "  Found existing installation: Keras 2.3.1\n",
      "    Uninstalling Keras-2.3.1:\n",
      "      Successfully uninstalled Keras-2.3.1\n",
      "Successfully installed keras-2.2.4\n",
      "Requirement already satisfied: keras-rl in /usr/local/lib/python2.7/dist-packages (0.4.2)\n",
      "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python2.7/dist-packages (from keras-rl) (2.2.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from keras>=2.0.7->keras-rl) (1.16.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from keras>=2.0.7->keras-rl) (1.2.2)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from keras>=2.0.7->keras-rl) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "'''Just saying, Tensorflow & keras management of versions are something terrible'''\n",
    "\n",
    "# !pip uninstall tensorflow\n",
    "# !pip uninstall keras\n",
    "!pip install tensorflow==1.13.1\n",
    "!pip install keras==2.2.4\n",
    "\n",
    "# Uncomment this line for installing keras-rl on Google collaboratory\n",
    "!pip install keras-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJc7SKhbtPU0",
    "outputId": "d680280a-440e-46f8-89e3-e38ae8b83901"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlCYbnzktPU2"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAtPVrEptPU2"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcl6vjc6tPU3"
   },
   "source": [
    "1) Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Mvt6VG6tPU3",
    "outputId": "f77feb01-b206-48aa-b655-960a3a44bc8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "(4, 84, 84)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4))`\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), strides=(2, 2))`\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(1, 1))`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH,)+INPUT_SHAPE\n",
    "\n",
    "# this structure comes after some research on the internet, mainly https://yilundu.github.io/2016/12/24/Deep-Q-Learning-on-Space-Invaders.html\n",
    "model = Sequential()\n",
    "model.add(Permute((2,3,1), input_shape=input_shape))\n",
    "model.add(Convolution2D(32, 8, 8, subsample=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 4, 4, subsample=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3, subsample=(1,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "print (input_shape)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_RTmuF6tPU3"
   },
   "source": [
    "2) Selección de hiperparámetros para la solución DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndM_FTQFtPU4"
   },
   "outputs": [],
   "source": [
    "# TODO - Select the parameters for the memory\n",
    "\n",
    "memory = SequentialMemory(limit=200000, \n",
    "                          window_length=WINDOW_LENGTH) \n",
    "                          #https://www.oreilly.com/library/view/keras-deep-learning/9781788621755/8faadfd1-d5ae-44fc-bb71-9861c77262a5.xhtml\n",
    "processor = AtariProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stcHhj94tPU4"
   },
   "outputs": [],
   "source": [
    "# TODO - Select the parameters for the policy\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps',\n",
    "                              value_max=1., \n",
    "                              value_min=.1, \n",
    "                              value_test=.05,\n",
    "                              nb_steps=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqthlW5gtPU4"
   },
   "outputs": [],
   "source": [
    "# TODO - Select the parameters for the Agent and the Optimizer\n",
    "\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=nb_actions, \n",
    "               policy=policy,\n",
    "               memory=memory, \n",
    "               processor=processor,\n",
    "               nb_steps_warmup=50000, \n",
    "               gamma=0.99,\n",
    "               target_model_update=10000,\n",
    "               train_interval=20)\n",
    "\n",
    "dqn.compile(Adam(lr=.0005), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5LCod5RtPU4",
    "outputId": "2baa85a1-e2de-4b7f-98e8-e9855c748579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 900000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 55s 5ms/step - reward: 0.0136\n",
      "12 episodes - episode_reward: 10.583 [5.000, 21.000] - ale.lives: 2.092\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 0.0133\n",
      "14 episodes - episode_reward: 9.929 [2.000, 24.000] - ale.lives: 2.079\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: 0.0147\n",
      "13 episodes - episode_reward: 10.692 [5.000, 22.000] - ale.lives: 2.061\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 0.0136\n",
      "14 episodes - episode_reward: 10.000 [6.000, 15.000] - ale.lives: 2.055\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 55s 6ms/step - reward: 0.0164\n",
      "14 episodes - episode_reward: 11.857 [5.000, 22.000] - ale.lives: 2.022\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "   19/10000 [..............................] - ETA: 1:07 - reward: 0.0526    WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 0.0145\n",
      "14 episodes - episode_reward: 10.214 [6.000, 21.000] - loss: 0.007 - mean_absolute_error: 0.425 - mean_q: 0.513 - mean_eps: 0.752 - ale.lives: 2.122\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 145s 14ms/step - reward: 0.0134\n",
      "15 episodes - episode_reward: 9.133 [4.000, 14.000] - loss: 0.007 - mean_absolute_error: 0.431 - mean_q: 0.519 - mean_eps: 0.708 - ale.lives: 1.992\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 144s 14ms/step - reward: 0.0141\n",
      "13 episodes - episode_reward: 10.462 [4.000, 20.000] - loss: 0.008 - mean_absolute_error: 0.440 - mean_q: 0.529 - mean_eps: 0.663 - ale.lives: 2.112\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 145s 15ms/step - reward: 0.0162\n",
      "14 episodes - episode_reward: 11.929 [4.000, 29.000] - loss: 0.008 - mean_absolute_error: 0.445 - mean_q: 0.535 - mean_eps: 0.618 - ale.lives: 1.935\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 144s 14ms/step - reward: 0.0156\n",
      "12 episodes - episode_reward: 12.917 [5.000, 27.000] - loss: 0.006 - mean_absolute_error: 0.448 - mean_q: 0.542 - mean_eps: 0.573 - ale.lives: 1.939\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 145s 14ms/step - reward: 0.0136\n",
      "13 episodes - episode_reward: 9.538 [2.000, 22.000] - loss: 0.008 - mean_absolute_error: 0.457 - mean_q: 0.553 - mean_eps: 0.528 - ale.lives: 1.837\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 145s 14ms/step - reward: 0.0181\n",
      "13 episodes - episode_reward: 14.000 [5.000, 22.000] - loss: 0.007 - mean_absolute_error: 0.464 - mean_q: 0.560 - mean_eps: 0.483 - ale.lives: 2.054\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 0.0151\n",
      "14 episodes - episode_reward: 11.571 [3.000, 23.000] - loss: 0.007 - mean_absolute_error: 0.471 - mean_q: 0.570 - mean_eps: 0.438 - ale.lives: 2.138\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 144s 14ms/step - reward: 0.0175\n",
      "14 episodes - episode_reward: 12.357 [4.000, 21.000] - loss: 0.008 - mean_absolute_error: 0.482 - mean_q: 0.583 - mean_eps: 0.393 - ale.lives: 2.066\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 144s 14ms/step - reward: 0.0168\n",
      "12 episodes - episode_reward: 13.833 [8.000, 21.000] - loss: 0.007 - mean_absolute_error: 0.484 - mean_q: 0.584 - mean_eps: 0.348 - ale.lives: 1.914\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 0.0145\n",
      "11 episodes - episode_reward: 13.182 [4.000, 36.000] - loss: 0.008 - mean_absolute_error: 0.492 - mean_q: 0.593 - mean_eps: 0.303 - ale.lives: 2.110\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 145s 15ms/step - reward: 0.0192\n",
      "13 episodes - episode_reward: 14.615 [5.000, 28.000] - loss: 0.008 - mean_absolute_error: 0.501 - mean_q: 0.606 - mean_eps: 0.258 - ale.lives: 2.066\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 141s 14ms/step - reward: 0.0167\n",
      "13 episodes - episode_reward: 13.308 [6.000, 20.000] - loss: 0.008 - mean_absolute_error: 0.506 - mean_q: 0.612 - mean_eps: 0.213 - ale.lives: 2.064\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 142s 14ms/step - reward: 0.0175\n",
      "13 episodes - episode_reward: 13.308 [6.000, 27.000] - loss: 0.008 - mean_absolute_error: 0.518 - mean_q: 0.625 - mean_eps: 0.168 - ale.lives: 2.107\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0175\n",
      "11 episodes - episode_reward: 14.091 [3.000, 30.000] - loss: 0.008 - mean_absolute_error: 0.527 - mean_q: 0.637 - mean_eps: 0.123 - ale.lives: 1.926\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 0.0172\n",
      "13 episodes - episode_reward: 14.615 [7.000, 27.000] - loss: 0.010 - mean_absolute_error: 0.559 - mean_q: 0.674 - mean_eps: 0.100 - ale.lives: 1.885\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 140s 14ms/step - reward: 0.0130\n",
      "14 episodes - episode_reward: 9.857 [4.000, 19.000] - loss: 0.008 - mean_absolute_error: 0.572 - mean_q: 0.690 - mean_eps: 0.100 - ale.lives: 2.014\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 141s 14ms/step - reward: 0.0189\n",
      "14 episodes - episode_reward: 13.500 [6.000, 23.000] - loss: 0.008 - mean_absolute_error: 0.589 - mean_q: 0.710 - mean_eps: 0.100 - ale.lives: 2.030\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 144s 14ms/step - reward: 0.0169\n",
      "13 episodes - episode_reward: 12.538 [4.000, 28.000] - loss: 0.009 - mean_absolute_error: 0.604 - mean_q: 0.727 - mean_eps: 0.100 - ale.lives: 2.100\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 0.0175\n",
      "14 episodes - episode_reward: 12.929 [6.000, 36.000] - loss: 0.009 - mean_absolute_error: 0.613 - mean_q: 0.739 - mean_eps: 0.100 - ale.lives: 1.978\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 152s 15ms/step - reward: 0.0174\n",
      "11 episodes - episode_reward: 15.000 [6.000, 23.000] - loss: 0.010 - mean_absolute_error: 0.626 - mean_q: 0.754 - mean_eps: 0.100 - ale.lives: 2.147\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0184\n",
      "13 episodes - episode_reward: 14.077 [6.000, 24.000] - loss: 0.008 - mean_absolute_error: 0.638 - mean_q: 0.769 - mean_eps: 0.100 - ale.lives: 2.096\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0154\n",
      "9 episodes - episode_reward: 16.111 [11.000, 25.000] - loss: 0.008 - mean_absolute_error: 0.642 - mean_q: 0.774 - mean_eps: 0.100 - ale.lives: 2.078\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0183\n",
      "12 episodes - episode_reward: 16.417 [6.000, 26.000] - loss: 0.010 - mean_absolute_error: 0.650 - mean_q: 0.782 - mean_eps: 0.100 - ale.lives: 2.007\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0196\n",
      "13 episodes - episode_reward: 14.615 [4.000, 34.000] - loss: 0.008 - mean_absolute_error: 0.648 - mean_q: 0.780 - mean_eps: 0.100 - ale.lives: 1.982\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0197\n",
      "14 episodes - episode_reward: 14.357 [6.000, 20.000] - loss: 0.009 - mean_absolute_error: 0.654 - mean_q: 0.787 - mean_eps: 0.100 - ale.lives: 2.067\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 0.0190\n",
      "13 episodes - episode_reward: 15.231 [6.000, 30.000] - loss: 0.010 - mean_absolute_error: 0.662 - mean_q: 0.797 - mean_eps: 0.100 - ale.lives: 1.903\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 0.0186\n",
      "13 episodes - episode_reward: 13.538 [10.000, 22.000] - loss: 0.009 - mean_absolute_error: 0.662 - mean_q: 0.796 - mean_eps: 0.100 - ale.lives: 2.036\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0168\n",
      "11 episodes - episode_reward: 15.727 [10.000, 26.000] - loss: 0.009 - mean_absolute_error: 0.668 - mean_q: 0.803 - mean_eps: 0.100 - ale.lives: 1.942\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 0.0160\n",
      "9 episodes - episode_reward: 18.333 [8.000, 32.000] - loss: 0.009 - mean_absolute_error: 0.666 - mean_q: 0.802 - mean_eps: 0.100 - ale.lives: 1.938\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0181\n",
      "10 episodes - episode_reward: 17.500 [10.000, 31.000] - loss: 0.008 - mean_absolute_error: 0.674 - mean_q: 0.810 - mean_eps: 0.100 - ale.lives: 1.909\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 0.0180\n",
      "12 episodes - episode_reward: 14.250 [4.000, 24.000] - loss: 0.008 - mean_absolute_error: 0.679 - mean_q: 0.817 - mean_eps: 0.100 - ale.lives: 2.011\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 0.0205\n",
      "14 episodes - episode_reward: 15.643 [3.000, 27.000] - loss: 0.009 - mean_absolute_error: 0.682 - mean_q: 0.820 - mean_eps: 0.100 - ale.lives: 1.944\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 145s 15ms/step - reward: 0.0185\n",
      "11 episodes - episode_reward: 14.636 [9.000, 34.000] - loss: 0.009 - mean_absolute_error: 0.688 - mean_q: 0.827 - mean_eps: 0.100 - ale.lives: 1.948\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 0.0174\n",
      "11 episodes - episode_reward: 17.636 [7.000, 27.000] - loss: 0.009 - mean_absolute_error: 0.690 - mean_q: 0.829 - mean_eps: 0.100 - ale.lives: 1.947\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 0.0213\n",
      "13 episodes - episode_reward: 15.923 [7.000, 23.000] - loss: 0.010 - mean_absolute_error: 0.690 - mean_q: 0.829 - mean_eps: 0.100 - ale.lives: 2.047\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0185\n",
      "13 episodes - episode_reward: 15.000 [8.000, 23.000] - loss: 0.010 - mean_absolute_error: 0.693 - mean_q: 0.833 - mean_eps: 0.100 - ale.lives: 2.095\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 145s 14ms/step - reward: 0.0231\n",
      "12 episodes - episode_reward: 18.917 [10.000, 29.000] - loss: 0.010 - mean_absolute_error: 0.696 - mean_q: 0.836 - mean_eps: 0.100 - ale.lives: 1.966\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 144s 14ms/step - reward: 0.0194\n",
      "10 episodes - episode_reward: 18.800 [8.000, 30.000] - loss: 0.009 - mean_absolute_error: 0.704 - mean_q: 0.846 - mean_eps: 0.100 - ale.lives: 1.993\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 145s 14ms/step - reward: 0.0198\n",
      "13 episodes - episode_reward: 15.615 [9.000, 24.000] - loss: 0.010 - mean_absolute_error: 0.711 - mean_q: 0.853 - mean_eps: 0.100 - ale.lives: 2.083\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0206\n",
      "10 episodes - episode_reward: 19.800 [11.000, 27.000] - loss: 0.010 - mean_absolute_error: 0.721 - mean_q: 0.866 - mean_eps: 0.100 - ale.lives: 1.937\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 0.0210\n",
      "14 episodes - episode_reward: 16.000 [7.000, 26.000] - loss: 0.010 - mean_absolute_error: 0.723 - mean_q: 0.868 - mean_eps: 0.100 - ale.lives: 2.008\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 0.0216\n",
      "13 episodes - episode_reward: 16.077 [8.000, 29.000] - loss: 0.011 - mean_absolute_error: 0.724 - mean_q: 0.869 - mean_eps: 0.100 - ale.lives: 2.115\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0198\n",
      "10 episodes - episode_reward: 19.700 [8.000, 32.000] - loss: 0.010 - mean_absolute_error: 0.722 - mean_q: 0.867 - mean_eps: 0.100 - ale.lives: 2.002\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 0.0190\n",
      "10 episodes - episode_reward: 19.100 [9.000, 33.000] - loss: 0.010 - mean_absolute_error: 0.724 - mean_q: 0.868 - mean_eps: 0.100 - ale.lives: 1.922\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0193\n",
      "11 episodes - episode_reward: 17.273 [9.000, 23.000] - loss: 0.011 - mean_absolute_error: 0.726 - mean_q: 0.871 - mean_eps: 0.100 - ale.lives: 2.001\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0214\n",
      "13 episodes - episode_reward: 17.000 [6.000, 26.000] - loss: 0.011 - mean_absolute_error: 0.728 - mean_q: 0.873 - mean_eps: 0.100 - ale.lives: 1.947\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 15.538 [6.000, 21.000] - loss: 0.010 - mean_absolute_error: 0.729 - mean_q: 0.875 - mean_eps: 0.100 - ale.lives: 2.051\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0191\n",
      "13 episodes - episode_reward: 15.000 [5.000, 27.000] - loss: 0.010 - mean_absolute_error: 0.728 - mean_q: 0.873 - mean_eps: 0.100 - ale.lives: 1.889\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0206\n",
      "14 episodes - episode_reward: 14.714 [7.000, 21.000] - loss: 0.010 - mean_absolute_error: 0.730 - mean_q: 0.875 - mean_eps: 0.100 - ale.lives: 1.964\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0192\n",
      "14 episodes - episode_reward: 13.143 [6.000, 26.000] - loss: 0.011 - mean_absolute_error: 0.731 - mean_q: 0.876 - mean_eps: 0.100 - ale.lives: 2.031\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0208\n",
      "12 episodes - episode_reward: 16.833 [6.000, 28.000] - loss: 0.010 - mean_absolute_error: 0.732 - mean_q: 0.878 - mean_eps: 0.100 - ale.lives: 1.911\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0206\n",
      "13 episodes - episode_reward: 16.077 [8.000, 26.000] - loss: 0.010 - mean_absolute_error: 0.732 - mean_q: 0.878 - mean_eps: 0.100 - ale.lives: 2.043\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 146s 15ms/step - reward: 0.0193\n",
      "10 episodes - episode_reward: 20.400 [9.000, 30.000] - loss: 0.010 - mean_absolute_error: 0.736 - mean_q: 0.882 - mean_eps: 0.100 - ale.lives: 2.083\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 0.0201\n",
      "9 episodes - episode_reward: 20.000 [3.000, 34.000] - loss: 0.011 - mean_absolute_error: 0.739 - mean_q: 0.886 - mean_eps: 0.100 - ale.lives: 1.876\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0198\n",
      "12 episodes - episode_reward: 16.500 [6.000, 27.000] - loss: 0.010 - mean_absolute_error: 0.742 - mean_q: 0.890 - mean_eps: 0.100 - ale.lives: 2.022\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0227\n",
      "15 episodes - episode_reward: 15.467 [6.000, 30.000] - loss: 0.011 - mean_absolute_error: 0.743 - mean_q: 0.891 - mean_eps: 0.100 - ale.lives: 2.017\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0219\n",
      "13 episodes - episode_reward: 16.462 [10.000, 23.000] - loss: 0.011 - mean_absolute_error: 0.741 - mean_q: 0.888 - mean_eps: 0.100 - ale.lives: 1.938\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0216\n",
      "13 episodes - episode_reward: 17.462 [12.000, 29.000] - loss: 0.010 - mean_absolute_error: 0.742 - mean_q: 0.890 - mean_eps: 0.100 - ale.lives: 1.988\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0215\n",
      "13 episodes - episode_reward: 17.077 [5.000, 32.000] - loss: 0.010 - mean_absolute_error: 0.747 - mean_q: 0.897 - mean_eps: 0.100 - ale.lives: 2.014\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0210\n",
      "15 episodes - episode_reward: 13.867 [4.000, 19.000] - loss: 0.012 - mean_absolute_error: 0.749 - mean_q: 0.898 - mean_eps: 0.100 - ale.lives: 2.073\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0203\n",
      "14 episodes - episode_reward: 14.286 [4.000, 30.000] - loss: 0.010 - mean_absolute_error: 0.745 - mean_q: 0.893 - mean_eps: 0.100 - ale.lives: 2.037\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0216\n",
      "13 episodes - episode_reward: 15.923 [6.000, 35.000] - loss: 0.010 - mean_absolute_error: 0.741 - mean_q: 0.889 - mean_eps: 0.100 - ale.lives: 2.151\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0225\n",
      "11 episodes - episode_reward: 20.727 [12.000, 36.000] - loss: 0.010 - mean_absolute_error: 0.745 - mean_q: 0.894 - mean_eps: 0.100 - ale.lives: 2.014\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0226\n",
      "11 episodes - episode_reward: 21.273 [10.000, 34.000] - loss: 0.011 - mean_absolute_error: 0.749 - mean_q: 0.897 - mean_eps: 0.100 - ale.lives: 2.198\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0253\n",
      "12 episodes - episode_reward: 20.833 [14.000, 32.000] - loss: 0.010 - mean_absolute_error: 0.749 - mean_q: 0.898 - mean_eps: 0.100 - ale.lives: 2.002\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 0.0220\n",
      "13 episodes - episode_reward: 16.923 [6.000, 31.000] - loss: 0.011 - mean_absolute_error: 0.753 - mean_q: 0.903 - mean_eps: 0.100 - ale.lives: 2.019\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0258\n",
      "13 episodes - episode_reward: 20.077 [11.000, 29.000] - loss: 0.011 - mean_absolute_error: 0.756 - mean_q: 0.906 - mean_eps: 0.100 - ale.lives: 2.169\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 147s 15ms/step - reward: 0.0217\n",
      "15 episodes - episode_reward: 14.867 [5.000, 33.000] - loss: 0.011 - mean_absolute_error: 0.758 - mean_q: 0.909 - mean_eps: 0.100 - ale.lives: 2.047\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0255\n",
      "13 episodes - episode_reward: 18.385 [10.000, 28.000] - loss: 0.012 - mean_absolute_error: 0.758 - mean_q: 0.908 - mean_eps: 0.100 - ale.lives: 2.096\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 0.0237\n",
      "18 episodes - episode_reward: 13.556 [5.000, 26.000] - loss: 0.011 - mean_absolute_error: 0.760 - mean_q: 0.910 - mean_eps: 0.100 - ale.lives: 2.138\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0235\n",
      "16 episodes - episode_reward: 14.875 [9.000, 27.000] - loss: 0.012 - mean_absolute_error: 0.763 - mean_q: 0.914 - mean_eps: 0.100 - ale.lives: 1.962\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 0.0203\n",
      "15 episodes - episode_reward: 13.400 [6.000, 18.000] - loss: 0.011 - mean_absolute_error: 0.762 - mean_q: 0.914 - mean_eps: 0.100 - ale.lives: 1.949\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 152s 15ms/step - reward: 0.0217\n",
      "15 episodes - episode_reward: 13.667 [6.000, 23.000] - loss: 0.012 - mean_absolute_error: 0.769 - mean_q: 0.921 - mean_eps: 0.100 - ale.lives: 1.891\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 152s 15ms/step - reward: 0.0183\n",
      "17 episodes - episode_reward: 11.471 [6.000, 22.000] - loss: 0.011 - mean_absolute_error: 0.771 - mean_q: 0.923 - mean_eps: 0.100 - ale.lives: 2.016\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 151s 15ms/step - reward: 0.0252\n",
      "15 episodes - episode_reward: 16.800 [9.000, 25.000] - loss: 0.011 - mean_absolute_error: 0.772 - mean_q: 0.924 - mean_eps: 0.100 - ale.lives: 1.979\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 152s 15ms/step - reward: 0.0218\n",
      "17 episodes - episode_reward: 13.118 [6.000, 20.000] - loss: 0.013 - mean_absolute_error: 0.776 - mean_q: 0.928 - mean_eps: 0.100 - ale.lives: 1.948\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 151s 15ms/step - reward: 0.0222\n",
      "15 episodes - episode_reward: 14.067 [7.000, 24.000] - loss: 0.013 - mean_absolute_error: 0.776 - mean_q: 0.928 - mean_eps: 0.100 - ale.lives: 1.928\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 151s 15ms/step - reward: 0.0215\n",
      "16 episodes - episode_reward: 14.250 [6.000, 29.000] - loss: 0.011 - mean_absolute_error: 0.776 - mean_q: 0.929 - mean_eps: 0.100 - ale.lives: 2.031\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 152s 15ms/step - reward: 0.0223\n",
      "15 episodes - episode_reward: 14.600 [6.000, 22.000] - loss: 0.012 - mean_absolute_error: 0.777 - mean_q: 0.929 - mean_eps: 0.100 - ale.lives: 1.982\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 151s 15ms/step - reward: 0.0191\n",
      "13 episodes - episode_reward: 14.231 [6.000, 22.000] - loss: 0.013 - mean_absolute_error: 0.779 - mean_q: 0.931 - mean_eps: 0.100 - ale.lives: 1.979\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 150s 15ms/step - reward: 0.0198\n",
      "12 episodes - episode_reward: 17.000 [8.000, 26.000] - loss: 0.011 - mean_absolute_error: 0.777 - mean_q: 0.930 - mean_eps: 0.100 - ale.lives: 2.255\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 151s 15ms/step - reward: 0.0219\n",
      "12 episodes - episode_reward: 18.167 [11.000, 28.000] - loss: 0.012 - mean_absolute_error: 0.780 - mean_q: 0.933 - mean_eps: 0.100 - ale.lives: 1.896\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 149s 15ms/step - reward: 0.0215\n",
      "15 episodes - episode_reward: 14.067 [2.000, 27.000] - loss: 0.011 - mean_absolute_error: 0.780 - mean_q: 0.933 - mean_eps: 0.100 - ale.lives: 2.180\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 151s 15ms/step - reward: 0.0225\n",
      "done, took 12828.332 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training part\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "callbacks += [FileLogger(log_filename, interval=100)]\n",
    "\n",
    "# TODO - Select the parameters for the method \"fit\"\n",
    "dqn.fit(env, callbacks=callbacks, \n",
    "        nb_steps=900000, \n",
    "        log_interval=10000, \n",
    "        visualize=False)\n",
    "\n",
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pnv5GPqetPU5",
    "outputId": "754eab0f-b547-42fb-e9bd-e1eedefd0ffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 15.000, steps: 620\n",
      "Episode 2: reward: 16.000, steps: 951\n",
      "Episode 3: reward: 16.000, steps: 707\n",
      "Episode 4: reward: 17.000, steps: 1023\n",
      "Episode 5: reward: 18.000, steps: 904\n",
      "Episode 6: reward: 12.000, steps: 411\n",
      "Episode 7: reward: 23.000, steps: 1019\n",
      "Episode 8: reward: 9.000, steps: 509\n",
      "Episode 9: reward: 26.000, steps: 998\n",
      "Episode 10: reward: 14.000, steps: 533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9fd286dcd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing part to calculate the mean reward\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTyiY4dWtPU5"
   },
   "source": [
    "#### 3) Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4irCZfZTtfx"
   },
   "source": [
    "- Una vez construido el modelo, he intentado seguir las indicaciones de clase para modificar los hiperparámetros. Si bien es cierto que después he probado un par de configuraciones con parámetros ligeramente diferentes de forma estocástica.\n",
    "- Se ha conseguido superar el objetivo de reward."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Inm59eH6tPUc"
   ],
   "name": "task2-reinforcement_learning-2a_conv-Alberto_Vargas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "computer_vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
